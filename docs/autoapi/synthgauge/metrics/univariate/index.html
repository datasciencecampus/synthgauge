<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>synthgauge.metrics.univariate &mdash; SynthGauge 2.1.0 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
    <link rel="shortcut icon" href="../../../../_static/favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="synthgauge.datasets" href="../../datasets/index.html" />
    <link rel="prev" title="synthgauge.metrics.propensity" href="../propensity/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> SynthGauge
            <img src="../../../../_static/favicon.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../index.html">Home</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../demo.html">Example notebook</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge.metrics</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../index.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">SynthGauge</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">API Reference</a> &raquo;</li>
          <li><a href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge</span></code></a> &raquo;</li>
          <li><a href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge.metrics</span></code></a> &raquo;</li>
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge.metrics.univariate</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/synthgauge/metrics/univariate/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-synthgauge.metrics.univariate">
<span id="synthgauge-metrics-univariate"></span><h1><a class="reference internal" href="#module-synthgauge.metrics.univariate" title="synthgauge.metrics.univariate"><code class="xref py py-mod docutils literal notranslate"><span class="pre">synthgauge.metrics.univariate</span></code></a><a class="headerlink" href="#module-synthgauge.metrics.univariate" title="Permalink to this heading"></a></h1>
<p>Univariate utility metrics.</p>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading"></a></h2>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.kullback_leibler" title="synthgauge.metrics.univariate.kullback_leibler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kullback_leibler</span></code></a>(real, synth, feature[, bins])</p></td>
<td><p>Kullback-Leibler divergence.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.jensen_shannon_divergence" title="synthgauge.metrics.univariate.jensen_shannon_divergence"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jensen_shannon_divergence</span></code></a>(real, synth, feature[, bins])</p></td>
<td><p>Jensen-Shannon divergence.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.jensen_shannon_distance" title="synthgauge.metrics.univariate.jensen_shannon_distance"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jensen_shannon_distance</span></code></a>(real, synth, feature[, bins])</p></td>
<td><p>Jensen-Shannon distance.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.wasserstein" title="synthgauge.metrics.univariate.wasserstein"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wasserstein</span></code></a>(real, synth, feature, **kwargs)</p></td>
<td><p>The (first) Wasserstein distance.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.kolmogorov_smirnov" title="synthgauge.metrics.univariate.kolmogorov_smirnov"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kolmogorov_smirnov</span></code></a>(real, synth, feature, **kwargs)</p></td>
<td><p>Kolmogorov-Smirnov test.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.kruskal_wallis" title="synthgauge.metrics.univariate.kruskal_wallis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kruskal_wallis</span></code></a>(real, synth, feature, **kwargs)</p></td>
<td><p>Kruskal-Wallis H test.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.mann_whitney" title="synthgauge.metrics.univariate.mann_whitney"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mann_whitney</span></code></a>(real, synth, feature, **kwargs)</p></td>
<td><p>Mann-Whitney U test.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#synthgauge.metrics.univariate.wilcoxon" title="synthgauge.metrics.univariate.wilcoxon"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wilcoxon</span></code></a>(real, synth, feature, **kwargs)</p></td>
<td><p>Wilcoxon signed-rank test.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.kullback_leibler">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">kullback_leibler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#kullback_leibler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.kullback_leibler" title="Permalink to this definition"></a></dt>
<dd><p>Kullback-Leibler divergence.</p>
<p>Describes how much the synthetic feature distribution varies from
the real distribution in terms of relative entropy. The divergence
is assymmetric and does not satisfy the triangle inequality. Thus,
it does not describe “distance” in the mathematical sense.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>bins</strong> (<em>int</em><em> or </em><em>str</em><em> or </em><em>None</em><em>, </em><em>default &quot;auto&quot;</em>) – The binning method to use. If <cite>int</cite>, is the number of bins. If
<cite>str</cite>, must be a method accepted by <cite>numpy.histogram_bin_edges</cite>.
If <cite>None</cite>, the feature is assumed to be categorical and counts
are taken for each value in either dataset.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.entropy</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The computed divergence between the distributions.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.entropy</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper function for <cite>scipy.stats.entropy</cite>. Since this
function expects a probability vector, the data is first discretised
into evenly-spaced bins.</p>
<p>We can think of the Kullback-Leibler divergence as a measure of
surprise we might expect seeing an example from the real data,
relative to the distribution of the synthetic.</p>
<p>The divergence is zero if the distributions are identical, and
larger values indicate that the two discretised distributions are
further from one another. Note that the KL divergence may be
infinite - the most common case where this happens is when the
synthetic data does not contain any data in at least one bin.</p>
<p>An optimal ‘bins’ value has not been suggested.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;emu&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The first feature is replicated up to a re-ordering in the synthetic
data, so its KL divergence is zero:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kullback_leibler</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
<p>However, the second feature does not include the <code class="docutils literal notranslate"><span class="pre">&quot;emu&quot;</span></code> category
in the synthetic data, so the divergence is infinite:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kullback_leibler</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">inf</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.jensen_shannon_divergence">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">jensen_shannon_divergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#jensen_shannon_divergence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.jensen_shannon_divergence" title="Permalink to this definition"></a></dt>
<dd><p>Jensen-Shannon divergence.</p>
<p>Also known as the information radius, the Jensen-Shannon divergence
describes the similarity between two probability distributions in
terms of entropy. This divergence modifies the Kullback-Leibler
divergence to be symmetric and finite (between 0 and 1).</p>
<p>The divergence does not satisfy the triangle inequality. Thus, it
does not describe “distance” in the mathematical sense. Taking its
square root provides a metric known as the Jensen-Shannon distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>bins</strong> (<em>int</em><em> or </em><em>str</em><em> or </em><em>None</em><em>, </em><em>default &quot;auto&quot;</em>) – The binning method to use. If <cite>int</cite>, is the number of bins. If
<cite>str</cite>, must be a method accepted by <cite>numpy.histogram_bin_edges</cite>.
If <cite>None</cite>, the feature is assumed to be categorical and counts
are taken for each value in either dataset.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.spatial.distance.jensenshannon</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The computed divergence between the distributions.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">synthgauge.metrics.univariate_distance.jensen_shannon_distance</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.spatial.distance.jensenshannon</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper of
<cite>synthgauge.metrics.univariate_distance.jensen_shannon_distance</cite>,
which in turn wraps <cite>scipy.spatial.distance.jensenshannon</cite>. Since
this function expects probability vectors, the data is first
discretised into evenly-spaced bins.</p>
<p>We can think of the Jensen-Shannon divergence as the amount of
information, or entropy, encoded in the difference between the
real and synthetic distributions of the feature.</p>
<p>The divergence is zero if the distributions are identical, and is
bounded above by one if they are nothing alike. This method is
therefore good for comparing multiple synthetic datasets, or
features within a dataset, to see which is closest to the real.
However, as this is not a test, there is no threshold distance below
which we can claim the distributions are statistically the same.</p>
<p>An optimal ‘bins’ value has not been suggested.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;emu&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The second feature appears to be more similar than the first across
datasets:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jensen_shannon_divergence</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.1732867951399863</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jensen_shannon_divergence</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">0.10788077716941784</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.jensen_shannon_distance">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">jensen_shannon_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#jensen_shannon_distance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.jensen_shannon_distance" title="Permalink to this definition"></a></dt>
<dd><p>Jensen-Shannon distance.</p>
<p>Describes the difference between two distributions in terms of
entropy. Calculated as the square root of the Jensen-Shannon
divergence, the Jensen-Shannon distance satisfies the mathematical
definition of a metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>bins</strong> (<em>int</em><em> or </em><em>str</em><em> or </em><em>None</em><em>, </em><em>default &quot;auto&quot;</em>) – The binning method to use. If <cite>int</cite>, is the number of bins. If
<cite>str</cite>, must be a method accepted by <cite>numpy.histogram_bin_edges</cite>.
If <cite>None</cite>, the feature is assumed to be categorical and counts
are taken for each value in either dataset.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.spatial.distance.jensenshannon</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distance</strong> – The computed distance between the distributions.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">synthgauge.metrics.univariate_distance.jensen_shannon_divergence</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.spatial.distance.jensenshannon</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper for <cite>scipy.spatial.distance.jensenshannon</cite>. Since
this function expects probability vectors, the data is first
discretised into evenly-spaced bins.</p>
<p>We can think of the Jensen-Shannon distance as the amount of
information, or entropy, encoded in the difference between the
<cite>real</cite> and <cite>synth</cite> distributions of the <cite>feature</cite>.</p>
<p>The distance is zero if the distributions are identical, and is
bounded above by one if they are nothing alike. This method is
therefore good for comparing multiple synthetic datasets, or
features within a dataset, to see which is closest to the real.
However, as this is not a test, there is no threshold distance below
which we can claim the distributions are statistically the same.</p>
<p>An optimal ‘bins’ value has not been suggested.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;emu&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cow&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The second feature appears to be more similar than the first across
datasets:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jensen_shannon_distance</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.41627730557884884</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jensen_shannon_distance</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">0.328452092654953</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.wasserstein">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">wasserstein</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#wasserstein"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.wasserstein" title="Permalink to this definition"></a></dt>
<dd><p>The (first) Wasserstein distance.</p>
<p>Also known as the “Earth Mover’s” distance, this metric can be
thought of as calculating the amount of “work” required to move from
the distribution of the synthetic data to the distribution of the
real data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.wasserstein_distance</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The computed distance between the distributions.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.wasserstein_distance</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper for <cite>scipy.stats.wasserstein_distance</cite>.
Computationally, we can find the Wasserstein distance by calculating
the area between the cumulative distribution functions for the two
distributions.</p>
<p>If <span class="math notranslate nohighlight">\(s\)</span> is the synthetic feature distribution, <span class="math notranslate nohighlight">\(r\)</span> is the
real feature distribution, and <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(S\)</span> are their
respective cumulative distribution functions, then</p>
<div class="math notranslate nohighlight">
\[W(s, r) = \int_{-\infty}^{+\infty} |S - R|\]</div>
<p>The distance is zero if the distributions are identical and
increases as they become less alike. This method is therefore good
for comparing multiple synthetic datasets, or features within a
dataset, to see which is closest to the real. However, as this is
not a test, there is no threshold distance below which we can claim
the distributions are statistically the same.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The first feature appears to be more similar than the second across
datasets:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">wasserstein</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.6666666666666667</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wasserstein</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="go">1.166666666666667</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.kolmogorov_smirnov">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">kolmogorov_smirnov</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#kolmogorov_smirnov"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.kolmogorov_smirnov" title="Permalink to this definition"></a></dt>
<dd><p>Kolmogorov-Smirnov test.</p>
<p>The Kolmogorov-Smirnov test statistic is the maximum difference
between the cumulative distribution functions of the real and
synthetic features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Name of the feature to compare. This must be continuous.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.ks_2samp</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>statistic, pvalue</strong> (<em>float</em>) – Kolmogorov-Smirnov test statistic.</p></li>
<li><p><strong>pvalue</strong> (<em>float</em>) – Two-tailed p-value.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.ks_2samp</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper for <cite>scipy.stats.ks_2samp</cite>, which tests whether
two samples are drawn from the same distribution by calculating the
maximum difference between their cumulative distribution functions.</p>
<p>If the returned statistic is small or the p-value is high, then we
cannot reject the hypothesis that the distributions are the same.</p>
<p>This approach is only defined if the feature is continuous. The
documentation further suggests this method works best when one of
the samples has a size of only a few thousand.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The first feature appears to come from the same distribution in both
datasets.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kolmogorov_smirnov</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">KstestResult(statistic=0.3333333333333333, pvalue=0.9307359307359307)</span>
</pre></div>
</div>
<p>The second feature appears to come from different distributions in
the datasets.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kolmogorov_smirnov</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="go">KstestResult(statistic=0.8333333333333334, pvalue=0.025974025974025972)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.kruskal_wallis">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">kruskal_wallis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#kruskal_wallis"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.kruskal_wallis" title="Permalink to this definition"></a></dt>
<dd><p>Kruskal-Wallis H test.</p>
<p>The Kruskal-Wallis test seeks to determine whether two sets of data
originated from the same distribution. This is acheived by pooling
and ranking the datasets. A low p-value suggests the two sets
originate from different distributions and are not similar.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.kruskal</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>statistic</strong> (<em>float</em>) – The Kruskal-Wallis H statistic.</p></li>
<li><p><strong>pvalue</strong> (<em>float</em>) – The p-value for the test.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.kruskal</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper function for <cite>scipy.stats.kruskal</cite>.</p>
<p>The null hypothesis for this test is that the medians of the
distributions are equal. The alternative hypothesis is then that
they are different. This would suggest that the synthetic and real
data are not similarly distributed.</p>
<p>We notice, however, that failure to reject the null hypothesis only
suggests that the medians could be equal and says nothing else about
how the data are distributed.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The test for the first feature suggests that the data are similarly
distributed according to their medians.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kruskal_wallis</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">KruskalResult(statistic=0.5646387832699667, pvalue=0.45239722100817814)</span>
</pre></div>
</div>
<p>The second feature test is much clearer that the data are drawn from
distributions with different medians.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kruskal_wallis</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="go">KruskalResult(statistic=4.877737226277376, pvalue=0.02720526089960062)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.mann_whitney">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">mann_whitney</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#mann_whitney"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.mann_whitney" title="Permalink to this definition"></a></dt>
<dd><p>Mann-Whitney U test.</p>
<p>The Mann-Whitney test compares two sets of data by examining how
well-mixed they are when pooled. This is acheived by ranking the
pooled data. A low p-value suggests the data are not similar.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.mannwhitneyu</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>statistic</strong> (<em>float</em>) – The Mann-Whitney U statistic, in particular U for <cite>synth</cite>.</p></li>
<li><p><strong>pvalue</strong> (<em>float</em>) – Two-sided p-value assuming an asymptotic normal distribution.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.mannwhitneyu</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper function for <cite>scipy.stats.mannwhitneyu</cite>.</p>
<p>The null hypothesis for this test is that for randomly selected real
and synthetic values, the probability that the real value is greater
than the synthetic is the same as the probability that the synthetic
value is greater than the real.</p>
<p>We reject this hypothesis if the p-value is suitably small. This
would in turn suggest that the synthetic and real data are not
similarly distributed.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>If we were to choose our p-value threshold as 0.05, we would reach
the conclusion that the distributions of the first feature are
similar but the distributions of the second feature are not.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mann_whitney</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">MannwhitneyuResult(statistic=22.5, pvalue=0.5041764308016705)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mann_whitney</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="go">MannwhitneyuResult(statistic=31.5, pvalue=0.033439907088311766)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="synthgauge.metrics.univariate.wilcoxon">
<span class="sig-prename descclassname"><span class="pre">synthgauge.metrics.univariate.</span></span><span class="sig-name descname"><span class="pre">wilcoxon</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">real</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/synthgauge/metrics/univariate.html#wilcoxon"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#synthgauge.metrics.univariate.wilcoxon" title="Permalink to this definition"></a></dt>
<dd><p>Wilcoxon signed-rank test.</p>
<p>In this use, the Wilcoxon test compares the distributions of paired
data. It does this by ranking the pairwise differences between the
real and synthetic data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>real</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the real data.</p></li>
<li><p><strong>synth</strong> (<em>pandas.DataFrame</em>) – Dataframe containing the synthetic data.</p></li>
<li><p><strong>feature</strong> (<em>str</em>) – Feature of the datasets to compare. This must be continuous.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em><em>, </em><em>optional</em>) – Keyword arguments for <cite>scipy.stats.wilcoxon</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>statistic</strong> (<em>float</em>) – The sum of the ranks of the differences above or below zero,
whichever is greater.</p></li>
<li><p><strong>pvalue</strong> (<em>float</em>) – Two-sided p-value.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.wilcoxon</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>This is a wrapper function for <cite>scipy.stats.wilcoxon</cite>.</p>
<p>The null hypothesis for this test is that the median of the paired
differences is zero. The alternative hypothesis is that it is
different from zero. This would suggest that the synthetic and real
data are not similarly distributed.</p>
<p>This test only makes sense when the synthetic and real data are
paired. That is, each synthetic datum is matched to a real one. In
which case, it is required that data are ordered to reflect this.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">real</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">synth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]})</span>
</pre></div>
</div>
<p>By eye, you might think these distributions are quite different from
one another. However, the Wilcoxon test suggests that these two
datasets were drawn from similar distributions.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">wilcoxon</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">synth</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">WilcoxonResult(statistic=17.0, pvalue=0.9453125)</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../propensity/index.html" class="btn btn-neutral float-left" title="synthgauge.metrics.propensity" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../datasets/index.html" class="btn btn-neutral float-right" title="synthgauge.datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Data Science Campus.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>